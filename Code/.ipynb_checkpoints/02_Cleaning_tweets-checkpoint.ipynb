{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the main objective was to clean the tweets. That was one of the most important aspects at this point.\n",
    "The goal was to remove unnecessary characters and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from bs4 import BeautifulSoup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = pd.read_csv('../datasets/all_commodities_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>geo</th>\n",
       "      <th>place</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Jim_Howard_13 @RepAndyBiggsAZ @RandPaul just ...</td>\n",
       "      <td>1.260339e+18</td>\n",
       "      <td>corporationsgonewild üåä</td>\n",
       "      <td>any town USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Latest study of #Trump's proposed #COVID19 dru...</td>\n",
       "      <td>1.260334e+18</td>\n",
       "      <td>Darius</td>\n",
       "      <td>Berkeley, CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Restock!!\\n\\nClorox Fraganzia Multi-Purpose Cl...</td>\n",
       "      <td>1.260322e+18</td>\n",
       "      <td>Find üò∑ Essentials &amp; Save üí∞ Shopping Online</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @ABC7: Searching high and low for #Clorox d...</td>\n",
       "      <td>1.260316e+18</td>\n",
       "      <td>jeaned62803</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>üòä\\n[...] Alec #Baldwin returns as President #T...</td>\n",
       "      <td>1.260315e+18</td>\n",
       "      <td>Desnot</td>\n",
       "      <td>Paris Bastille</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6184</th>\n",
       "      <td>Scott Rapid-Dissolving Toilet Paper 8 rolls in...</td>\n",
       "      <td>1.257415e+18</td>\n",
       "      <td>Toilet Paper Tracker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xboxonelocator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6185</th>\n",
       "      <td>#Teachers, #Librarians &amp;amp; #Parents: Here's ...</td>\n",
       "      <td>1.257414e+18</td>\n",
       "      <td>Donna Gephart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6186</th>\n",
       "      <td>RT @ReinasWorld2020: @rebel_fla @deplorable_ru...</td>\n",
       "      <td>1.257413e+18</td>\n",
       "      <td>Hurricane</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6187</th>\n",
       "      <td>That explains a lot üòÇ                         ...</td>\n",
       "      <td>1.257412e+18</td>\n",
       "      <td>Jessica Weber</td>\n",
       "      <td>Ontario, Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6188</th>\n",
       "      <td>RT @ReinasWorld2020: @rebel_fla @deplorable_ru...</td>\n",
       "      <td>1.257412e+18</td>\n",
       "      <td>Melinda Roberts ‚≠ê‚≠ê‚≠ê</td>\n",
       "      <td>The Right</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6189 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets            id  \\\n",
       "0     @Jim_Howard_13 @RepAndyBiggsAZ @RandPaul just ...  1.260339e+18   \n",
       "1     Latest study of #Trump's proposed #COVID19 dru...  1.260334e+18   \n",
       "2     Restock!!\\n\\nClorox Fraganzia Multi-Purpose Cl...  1.260322e+18   \n",
       "3     RT @ABC7: Searching high and low for #Clorox d...  1.260316e+18   \n",
       "4     üòä\\n[...] Alec #Baldwin returns as President #T...  1.260315e+18   \n",
       "...                                                 ...           ...   \n",
       "6184  Scott Rapid-Dissolving Toilet Paper 8 rolls in...  1.257415e+18   \n",
       "6185  #Teachers, #Librarians &amp; #Parents: Here's ...  1.257414e+18   \n",
       "6186  RT @ReinasWorld2020: @rebel_fla @deplorable_ru...  1.257413e+18   \n",
       "6187  That explains a lot üòÇ                         ...  1.257412e+18   \n",
       "6188  RT @ReinasWorld2020: @rebel_fla @deplorable_ru...  1.257412e+18   \n",
       "\n",
       "                                            name         location coordinates  \\\n",
       "0                         corporationsgonewild üåä     any town USA         NaN   \n",
       "1                                         Darius     Berkeley, CA         NaN   \n",
       "2     Find üò∑ Essentials & Save üí∞ Shopping Online              NaN         NaN   \n",
       "3                                    jeaned62803              NaN         NaN   \n",
       "4                                         Desnot  Paris Bastille          NaN   \n",
       "...                                          ...              ...         ...   \n",
       "6184                        Toilet Paper Tracker              NaN         NaN   \n",
       "6185                               Donna Gephart              NaN         NaN   \n",
       "6186                                   Hurricane              NaN         NaN   \n",
       "6187                               Jessica Weber  Ontario, Canada         NaN   \n",
       "6188                         Melinda Roberts ‚≠ê‚≠ê‚≠ê        The Right         NaN   \n",
       "\n",
       "      created_at  favorite_count  geo place               source  \n",
       "0     2020-05-12             1.0  NaN   NaN      Twitter Web App  \n",
       "1     2020-05-12             0.0  NaN   NaN  Twitter for Android  \n",
       "2     2020-05-12             0.0  NaN   NaN      Twitter Web App  \n",
       "3     2020-05-12             0.0  NaN   NaN   Twitter for iPhone  \n",
       "4     2020-05-12             0.0  NaN   NaN      Twitter Web App  \n",
       "...          ...             ...  ...   ...                  ...  \n",
       "6184  2020-05-04             0.0  NaN   NaN       xboxonelocator  \n",
       "6185  2020-05-04             6.0  NaN   NaN      Twitter Web App  \n",
       "6186  2020-05-04             0.0  NaN   NaN  Twitter for Android  \n",
       "6187  2020-05-04             0.0  NaN   NaN            Instagram  \n",
       "6188  2020-05-04             0.0  NaN   NaN  Twitter for Android  \n",
       "\n",
       "[6189 rows x 10 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6189, 10)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Function to Clean the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function has been taken from Matt Brems NLP I lecture.\n",
    "# Only difference is a change in the function name\n",
    "# This function was used to clean the tweets\n",
    "def tweets_to_words(raw_text):\n",
    "    \n",
    "    # Calling on BeautifulSoup to work with the text\n",
    "    text = BeautifulSoup(raw_text).get_text()\n",
    "    \n",
    "    # Removing all characters and leaving only the letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Lowercasing the the words and splitting\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # Creating a stopwords set\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # Keeping the words that are not in the stopwords set\n",
    "    meaningful_words = [w for w in words if w not in stops]\n",
    "\n",
    "    # Bringing the words back together\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Restock!!\\n\\nClorox Fraganzia Multi-Purpose Cleaner, Forest Dew, 175 Ounces \\n\\nStarting at $14.52‚Ä¶ https://t.co/VknAtmUxZv'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the difference between and after the function\n",
    "all_tweets['tweets'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'restock clorox fraganzia multi purpose cleaner forest dew ounces starting https co vknatmuxzv'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_to_words(all_tweets['tweets'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tweets = all_tweets.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a for-loop over all Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does have a UserWarning. Because it was a warning about requesting from the 'https' I felt it was ok. There was no need to request any information from the links. Also the code was still able to function past this point. You are able to run from top to bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:389: UserWarning: \"https://t.co/Av4dNOU5ga\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:389: UserWarning: \"https://t.co/lZmHEP3OmQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets 1000 of 6189.\n",
      "Tweets 2000 of 6189.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:389: UserWarning: \"https://t.co/RXX4WIUJye\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets 3000 of 6189.\n",
      "Tweets 4000 of 6189.\n",
      "Tweets 5000 of 6189.\n",
      "Tweets 6000 of 6189.\n"
     ]
    }
   ],
   "source": [
    "# This code is from NLP I lesson by Matt Brems.\n",
    "# There have been some slight modifications made to fit this project.\n",
    "# Initialize an empty list to hold the clean titles.\n",
    "clean_tweets = []\n",
    "\n",
    "print(\"Cleaning and parsing the tweets...\")\n",
    "\n",
    "# Instantiate counter.\n",
    "j = 0\n",
    "\n",
    "# Looping through tweets...\n",
    "for tweet in all_tweets['tweets']:\n",
    "    \n",
    "    # Convert title to words, then append to clean_titles.\n",
    "    clean_tweets.append(tweets_to_words(tweet))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message.\n",
    "    if (j + 1) % 1000 == 0:\n",
    "        print(f'Tweets {j + 1} of {total_tweets}.')\n",
    "    \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning Cleaned Tweets into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets_df = pd.DataFrame(clean_tweets, columns=['clean_tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jim howard repandybiggsaz randpaul go drink cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latest study trump proposed covid drug hydroxy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>restock clorox fraganzia multi purpose cleaner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt abc searching high low clorox disinfecting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alec baldwin returns president trump drinking ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        clean_tweets\n",
       "0  jim howard repandybiggsaz randpaul go drink cl...\n",
       "1  latest study trump proposed covid drug hydroxy...\n",
       "2  restock clorox fraganzia multi purpose cleaner...\n",
       "3  rt abc searching high low clorox disinfecting ...\n",
       "4  alec baldwin returns president trump drinking ..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concating Clean Tweets onto Original Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = pd.concat([all_tweets, clean_tweets_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a New CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets.to_csv('datasets/cleaned_tweets.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
